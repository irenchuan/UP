{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./top.png\" alt=\"Alt\" style=\"width: 100%;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theory:0.5 Hour <br>\n",
    "Experiment:0.5 Hour<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers Overview\n",
    "\n",
    "Take a look at the image on top of the notebook! It shows a fun process: the sentence ‚ÄúI love Machine Learning!‚Äù gets broken down by a **Tokenizer** (a method that splits sentences into smaller, independent units). The sentence is divided into individual words and punctuation marks, which we call **tokens** (the smallest units of language). This is the very first step in processing text and, guess what? It‚Äôs the main topic of this course!\n",
    "\n",
    "But did you know? The concept of the tokenizer didn‚Äôt actually start with Natural Language Processing (NLP). It goes way back to the early days of computer programming! In the mid-20th century, computer scientists came up with the first tokenizers for splitting code into variable names, operators, and keywords (all called ‚Äútokens‚Äù). These tokens were analyzed and turned into machine-executable code. That was the humble beginning of tokenizers.\n",
    "\n",
    "Since then, tokenizers have been on an exciting journey from simple to complex methods. At first, **dictionary-based** and **rule-based tokenization** were the go-to approaches. But they had a big problem: when they ran into words they didn‚Äôt know (**OOV: Out-of-Vocabulary**), they got totally stuck!\n",
    "\n",
    "Then came **statistical tokenization**, which used **Token Frequency** (how often a word appears) to prioritize important words and make tokenization smarter. Soon after, **Subword tokenization** methods like **WordPiece** and **Unigram** stole the spotlight. They chopped long and complex words into smaller pieces, allowing models to understand words they‚Äôve never seen before by piecing together these smaller chunks.\n",
    "\n",
    "To help models know when sentences start and end, **Special Tokens** were introduced. And to make tokenization more efficient, **BPE (Byte-Pair Encoding)** came along, building small subword vocabularies that reduced the vocabulary size while keeping things fast.\n",
    "\n",
    "All these innovations work together to transform text into vectors that **Large Language Models**(**LLMs**) can understand, which go into the model‚Äôs **embedding layer** to help it better process and comprehend the text.\n",
    "\n",
    "In short, the evolution of tokenizers happened because of the real-world challenges they had to solve. Thanks to the brilliance and hard work of many geniuses, we now have advanced tools like **WordPiece**, **Unigram**, and **BPE**. So, let‚Äôs follow in their footsteps and dive into this magical journey together!\n",
    "\n",
    "---\n",
    "\n",
    "### **Tokenizer for Beginners: Let‚Äôs Have Some Fun!**\n",
    "\n",
    "Ready to jump into the world of tokenization? Let‚Äôs kick things off by working with a classic sentence that‚Äôs as famous as it is useful:\n",
    "\n",
    "**\"The quick brown fox jumps over the lazy dog.\"**\n",
    "\n",
    "This sentence is cool because it uses 26 letter of the alphabet! Perfect for showing you how to break things down. So, are you ready? Let‚Äôs dive in!\n",
    "\n",
    "---\n",
    "\n",
    "**What is Tokenization?**\n",
    "\n",
    "Think of tokenization as slicing up a big piece of text into bite-sized pieces called **tokens**. Tokens can be words, characters, or even whole sentences! It‚Äôs like taking a big cake and cutting it into smaller pieces ‚Äî easier to manage and a lot more fun to work with!\n",
    "\n",
    "---\n",
    "\n",
    "**Let‚Äôs Tokenize the Sentence!**\n",
    "\n",
    "Our sentence is:\n",
    "**\"The quick brown fox jumps over the lazy dog.\"**\n",
    "First, we can break it into **words**. This is the most basic type of tokenization:\n",
    "\n",
    "- **\"The\"**\n",
    "- **\"quick\"**\n",
    "- **\"brown\"**\n",
    "- **\"fox\"**\n",
    "- **\"jumps\"**\n",
    "- **\"over\"**\n",
    "- **\"the\"**\n",
    "- **\"lazy\"**\n",
    "- **\"dog\"**\n",
    "\n",
    "Boom! Now we have 9 tokens, each representing a word. Easy, right?\n",
    "\n",
    "---\n",
    "\n",
    "**Want to Level Up? How About Character Tokenization?**\n",
    "\n",
    "If you want to go deeper, you can tokenize by **characters**. In this case, each letter (and even spaces) becomes a token:\n",
    "\n",
    "- **\"T\"**\n",
    "- **\"h\"**\n",
    "- **\"e\"**\n",
    "- **\" \" (space)**\n",
    "- **\"q\"**\n",
    "- **\"u\"**\n",
    "- **\"i\"**\n",
    "- **\"c\"**\n",
    "- **\"k\"**\n",
    "- ‚Ä¶\n",
    "\n",
    "Now you‚Äôve got a super-detailed breakdown, where even the spaces are counted as tokens!\n",
    "\n",
    "---\n",
    "\n",
    "**Why Tokenization is Awesome**\n",
    "\n",
    "- **Makes text easier for computers to understand**: Computers don‚Äôt read sentences like we do, so tokens help them process language more effectively.\n",
    "- **Helps in tasks like search, translation, or predicting the next word**: Tokenization is the first step in many natural language processing (NLP) tasks.\n",
    "\n",
    "---\n",
    "\n",
    "See? Tokenization is like solving a fun puzzle! You break a sentence down into smaller, more manageable parts, and suddenly, text becomes something you can play with. Now that you‚Äôve mastered this classic example, you‚Äôre ready to tokenize anything! \n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts You Should Know\n",
    "\n",
    "Before we dive deeper into this course, let's take a quick break and recharge. Remember, \"sharpening the axe makes cutting wood easier!\"\n",
    "\n",
    "1. **Special Tokens**: The Magic Words for Language Models! üéâ\n",
    "\n",
    "Imagine you‚Äôre teaching a language model to read sentences, answer questions, or even classify emotions. To make sure it understands what‚Äôs going on, we use special tokens‚Äîkind of like secret code words that give the model instructions. Let‚Äôs check out a few of these magic tokens:\n",
    "\n",
    "`[s]`  : Think of this as a \"go!\" signal. It tells the model, ‚ÄúHey, a new sentence is starting!‚Äù\n",
    "`[/s]` : The stop sign! It tells the model, ‚ÄúOkay, sentence done!‚Äù\n",
    "`[CLS]`: This is like a boss token for tasks like emotion detection. It tells the model, ‚ÄúI‚Äôm here to classify something!‚Äù\n",
    "`[SEP]`: Like separating two sandwiches‚Äîthis token splits different text pieces apart. Great for things like question-answering.\n",
    "`[PAD]`: This token makes sure all input is the same size by adding blank space, kind of like stuffing extra pillows in a pillowcase!\n",
    "`[UNK]`: The ‚ÄúI don‚Äôt know this word‚Äù token. If the model can‚Äôt find a word in its vocabulary, it uses this.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Vocabulary**\n",
    "\n",
    "Think of a vocabulary as a giant bag of word snacks for a language model! Each snack is a token‚Äîcould be a word, part of a word, or even a character. The language model munches on these tokens to understand and create sentences. The model doesn‚Äôt \"read\" like us; it just breaks everything into tiny bite-sized tokens from its vocabulary. The bigger the vocabulary, the more tasty options the model has! So, when it processes your text, it grabs its tokens and builds sentences like a master word chef. Delicious, right?\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./Vocabulary.png\" alt=\"Alt\" style=\"width: 90%;\"/>\n",
    "        <p>Figure 1 - Vocabulary\n",
    "</div>\n",
    "\n",
    "The table above is the vocabulary for a language model. It shows how the model gives each word (token) a special number (TokenID)‚Äîkind of like a name tag! The model uses these numbers to figure out what each word is and how to process it. The word list in the table is embarrassingly small, but hey‚Äîit‚Äôs complete! To be honest, a model like this might not be very useful. Wait until I show you the example using it, I bet you‚Äôll realize it's trickier than it looks!\"\n",
    "\n",
    "Highlights:\n",
    "- **Common words** like \"excels\" and \"in\" get smaller numbers (1, 2) because the model sees them a lot.\n",
    "- **Less common words** like \"neuroscience\" get bigger numbers (9), since they‚Äôre not used as often.\n",
    "- **Even punctuation** gets a number (the period is 10), because punctuation matters too!\n",
    "\n",
    "When we talk about the difference between common and less common words, it introduces a concept called Token Frequency.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Token Frequency**\n",
    "\n",
    "Token frequency refers to how often a specific token appears in a dataset, which typically corresponds to the entire corpus used to train a language model. It's important because frequent tokens often play a central role in conveying the core meaning of sentences and are therefore treated differently. Tokenizers like Unigram take advantage of token frequency to build more efficient vocabularies by prioritizing the most common tokens.\n",
    "\n",
    "Since there are less common words in the vocabulary, there will also be words that have never appeared before. We have a special term for these words: 'OOV' (Out-of-Vocabulary).\n",
    "\n",
    "---\n",
    "\n",
    "4. **OOV (Out-of-Vocabulary)**\n",
    "\n",
    "OOV is like that one mysterious guest at the party that no one knows! When a word shows up that the model hasn‚Äôt learned yet, it‚Äôs labeled as OOV. The model doesn‚Äôt recognize it, so it needs to figure out how to handle this unexpected guest!\n",
    "In other words, if a word doesn‚Äôt exist in the model‚Äôs vocabulary, it‚Äôs considered **OOV**. In such cases, the model won‚Äôt understand the word, and it might be replaced by a placeholder token like `[UNK]` (unknown token).\n",
    "\n",
    "For example, the word 'quantum-computing' is an OOV (out-of-vocabulary) term for the vocabulary above, which refers to the field of study focused on developing computer technology based on the principles of quantum theory. \n",
    "\n",
    "When there is a sentence containing 'quantum-computing' and you want to input it into a language model that only recognizes the words in the vocabulary above, you need to replace it with`[UNK]`, otherwise the model won't understand the term and may fail to process the sentence correctly.\n",
    "\n",
    "Let's mix Special Tokens, Vocabulary, and OOV (Out-of-Vocabulary) with a cool example! Imagine we've got this sentence below:\n",
    "    \"MOHAMED BIN ZAYED UNIVERSITY excels in AI, excels in quantum-computing research neuroscience. \"\n",
    "    And we need to feed it into our language model, but the model only knows the words in the above limited vocabulary.\n",
    "    Let‚Äôs rebuild a vocabulary, this time including Special Tokens and handling OOV (Out-of-Vocabulary) words. üòÑ\n",
    "    \n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./Vocabulary with OOV.png\" alt=\"Alt\" style=\"width: 90%;\"/>\n",
    "    <p>Figure 2 - Vocabulary with special token and OOV\n",
    "</div>\n",
    "\n",
    "Special Token `[CLS]` and `[SEP]` usually have the highest frequency, so they are assigned the smallest TokenIDs (1 and 2). 'Excels' and 'in' are common words, so they follow next. \n",
    "The column **In Vocabulary?**: This column tells us whether a word is in the model‚Äôs vocabulary. If it is, it shows \"Yes\".\n",
    "The column **OOV**: This indicates which words are not in the model‚Äôs vocabulary, marked as OOV. For example, \"quantum-computing\" is not in the vocabulary, so its TokenID is `[UNK]`, which stands for an unknown word.\n",
    "This table shows how the model recognizes words, processes those that are in the vocabulary, and handles words that aren‚Äôt (replacing OOV words with `[UNK]`). \n",
    "These special tokens ensure that the model understands where a sentence starts and ends, how to classify certain parts of the text, and how to handle input sequences that have different lengths.\n",
    "It‚Äôs a great example for beginners to understand how a language model works. But think about it‚Äîif your vocabulary is really small, you‚Äôre going to end up with a lot of OOVs. When a sentence has too many OOVs, it becomes tricky for the model to understand its meaning. So, while OOVs help keep the model running, they don't solve the problem of accurate understanding, especially when dealing with a small vocabulary. And that's why the 'Subword Units' technique rose to the occasion!\n",
    "\n",
    "---\n",
    "\n",
    "5. **Subword Units**\n",
    "\n",
    "Sometimes, a word can be split into smaller parts, called subword units, to help the model better understand complex or unfamiliar words. This is especially helpful when the model comes across a word it hasn‚Äôt seen before. By breaking it down into recognizable pieces, the model can still make sense of the word, even if the full word isn‚Äôt in its vocabulary.\n",
    "\n",
    "We can break down the rare word \"neuroscience\" and the OOV word \"quantum-computing\" into smaller, meaningful parts. \n",
    "\n",
    "**neuroscience** can be split into: neuro + science.\"neuro\" refers to nerves or the nervous system (commonly used in fields like neuroscience, neurology).\"science\" is a common word that the model might already know.\n",
    "\n",
    "**quantum-computing** can be split into: quantum + - + computing.\"quantum\" refers to quantum mechanics, a specific area of physics.\"computing\" refers to the use of computers and technology.The hyphen (-) can also be treated as a separate subword unit.\n",
    "\n",
    "By splitting these words into Subword Units, the model can recognize and process each part individually, even if the full word isn't in the vocabulary. This way, the model can still understand the meaning of the words based on the subword pieces.\n",
    "\n",
    "---\n",
    "\n",
    "### Take a break\n",
    "\n",
    "So far, we‚Äôve covered the most important concepts of this lesson. How are you feeling? Up next, we‚Äôre going to use three different methods to handle the same sentence so you can really experience the subtle differences between them. Ready? If you‚Äôre not feeling super confident yet, feel free to review. But if you‚Äôre ready, let‚Äôs get going!\n",
    "\n",
    "---\n",
    "\n",
    "### The comparison between WordPiece, Unigram and BPE (Byte-Pair Encoding)\n",
    "\n",
    "To help beginners understand how different tokenization methods (WordPiece, Unigram, and BPE) break down the same sentence, let‚Äôs dive into a fun, beginner-friendly explanation and compare how each method works!\n",
    "\n",
    "The Sentence:\n",
    "\"MOHAMED BIN ZAYED UNIVERSITY excels in AI, in quantum-computing research neuroscience.\"\n",
    "\n",
    "Table: The tokenization results of the sentence using three different methods: WordPiece, Unigram, and BPE.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./Three tokenization methods.png\" alt=\"Alt\" style=\"width: 90%;\"/>\n",
    "    <p>Figure 3 - Three tokenization methods\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "1. **WordPiece** (Used in BERT):\n",
    "   - **Imagine WordPiece as a Lego set**: It breaks words into smaller chunks (called subword units), especially when it encounters rare or unfamiliar words. If the word is common, it remains intact; if it's uncommon or long, it gets split into smaller parts.\n",
    "   - For example, \"MOHAMED\" becomes **\"MO\"** + **\"##HAMED\"**, where **##** indicates that the second part is attached to the previous part. Similarly, \"neuroscience\" becomes **\"neuro\"** + **\"##science\"**.\n",
    "\n",
    "2. **Unigram** (Used in some multilingual models):\n",
    "   - **Unigram is like playing with full pieces of a puzzle**: It tries to represent words as whole pieces when possible, prioritizing the most common subwords in the vocabulary. If a word is common, it stays whole, but if it's rare, it gets split.\n",
    "   - For instance, \"MOHAMED\" remains **\"MOHAMED\"** since it's treated as a single piece, but it would split uncommon words like \"quantum-computing\" as a whole subword.\n",
    "\n",
    "3. **Byte-Pair Encoding (BPE)** (Used in GPT-2):\n",
    "   - **BPE is like combining building blocks**: It starts with individual characters and merges the most frequent pairs to form larger chunks. Common word parts are kept together, but less common or complex words get split into smaller chunks, much like WordPiece.\n",
    "   - For example, \"MOHAMED\" is split into **\"MO\"** + **\"##HAMED\"** just like WordPiece. Similarly, \"neuroscience\" becomes **\"neuro\"** + **\"##science\"**.\n",
    "\n",
    "---\n",
    "\n",
    "**Which Method Is Best?**\n",
    "- **WordPiece**: Efficient for breaking down rare or complex words, making it great for handling lots of vocabulary while maintaining meaning.\n",
    "- **Unigram**: Tries to keep common words whole but splits when necessary, useful for keeping languages more readable.\n",
    "- **BPE**: Works well by merging frequent word parts, making it flexible for both common and complex words.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**:\n",
    "Each method has its strengths and quirks. WordPiece and BPE are similar in how they split complex words into meaningful chunks, while Unigram keeps things more whole unless necessary. This table and explanation give you a feel for how models handle language differently with each approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model inputs**: from a sentence to an embedding vector\n",
    "\n",
    "The figure below shows the pipeline from a sentence to an embedding vector. **Tokenization** and **Embedding Vectors** are two key steps in NLP models. They play different roles in the process, but are so closely related that they‚Äôre like twin brothers. To help you understand better, I put them together in the figure to show you what happens before a sentence enters a language model. Let‚Äôs dive into it step by step.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./word2vetor.png\" alt=\"Alt\" style=\"width: 100%;\"/>\n",
    "    <p>Figure 4 - the pipeline from a sentence to an embedding vector\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "1. **What Tokenization Does**:\n",
    "   - **Tokenization** breaks down input text (a sentence or document) into smaller units called **tokens**. These tokens can be words, subwords, or even characters, depending on the tokenization method used (like WordPiece, BPE, or Unigram).\n",
    "   - The goal of tokenization is to convert the sentence into basic units that the model can work with and assign each token a **Token ID**, a unique number in the vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "2. **How Token IDs and Neural Networks Work Together**:\n",
    "   - The result of **Tokenization** is a series of **Token IDs**. These IDs are just numbers and can't be directly understood by the model, so they need to be processed further.\n",
    "   - **Token IDs** are passed through an embedding layer (usually the first layer of the neural network), which converts them into **Embedding Vectors**. These vectors are high-dimensional representations that capture the meaning and context of each token. This conversion process is done through an **Embedding Matrix**.\n",
    "\n",
    "---\n",
    "\n",
    "3. **From Tokenization to Embedding Vectors**:\n",
    "   - **Tokenization**: The sentence is split into tokens (like \"MOHAMED\", \"BIN\", \"excels\"), and each token is assigned a **Token ID**.\n",
    "   - **Neural Network Processing**: The **Token IDs** are passed to the model‚Äôs embedding layer, which uses an embedding matrix to convert the **Token IDs** into their corresponding **Embedding Vectors**.\n",
    "     - For example, Token ID 6 (\"MOHAMED\") might be mapped to a vector with 768 dimensions, capturing the meaning of \"MOHAMED\".\n",
    "   - **Understanding Embedding Dimensions**: Normally, embedding vectors have many dimensions (like 128, 256, or 768) to capture rich semantic information. But to make things easier here, we‚Äôre showing a **1-dimensional embedding** as an example. In reality, higher-dimensional vectors help the model understand tokens better.\n",
    "\n",
    "---\n",
    "\n",
    "4. **The Relationship Between Tokenization and Embedding**\n",
    "- **Tokenization** is the process of splitting a sentence into tokens and assigning them **Token IDs**‚Äîthis is the first step before the model can work with the text.\n",
    "- **Token IDs** are the result of tokenization and are then converted into **Embedding Vectors** through the embedding layer. This is the next step in how the model understands the text.\n",
    "- The relationship between **Tokenization** and **Embedding** is that tokenization provides the basic units (tokens), while embedding gives these units richer, higher-dimensional representations that the model can understand.\n",
    "\n",
    "In short, Tokenization breaks the text into pieces that the model can process, and Embedding, as the input to language models, converts those pieces into meaningful vectors, allowing the model to understand the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding and Practise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We‚Äôve finished the theory and examples for this lesson! Now, all that‚Äôs left is to quickly recreate the tokenizer results from Figure 3‚Äôs 'Three Tokenization Methods,' and then we‚Äôll wrap up this lesson!\n",
    "First of all, don‚Äôt forget to install the basic packages! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import the necessary Python packages for the program, and set up the sentence you want to process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "\n",
    "# Define the sentence\n",
    "sentence = \"MOHAMED BIN ZAYED UNIVERSITY excels in AI, in quantum-computing research neuroscience.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's get things rolling! When you're feeling confused, go to 'The comparison between WordPiece, Unigram, and BPE (Byte-Pair Encoding)' for the theory, and search Google for explanations of the code. The code is pretty simple, you can totally solve this on your own!\n",
    "\n",
    "---\n",
    "\n",
    "WordPiece Tokenizer goes first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordPiece Tokenizer\n",
    "def wordpiece_tokenizer():\n",
    "    tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    \n",
    "    # Train tokenizer on the sentence\n",
    "    trainer = trainers.WordPieceTrainer(vocab_size=100, special_tokens=[\"[CLS]\", \"[SEP]\", \"[UNK]\"])\n",
    "    tokenizer.train_from_iterator([sentence], trainer)\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    encoding = tokenizer.encode(sentence)\n",
    "    print(\"WordPiece Tokenization:\", encoding.tokens)\n",
    "    return encoding.tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unigram Tokenizer comes second!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram Tokenizer\n",
    "def unigram_tokenizer():\n",
    "    tokenizer = Tokenizer(models.Unigram())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    \n",
    "    # Train tokenizer on the sentence\n",
    "    trainer = trainers.UnigramTrainer(vocab_size=100, special_tokens=[\"[CLS]\", \"[SEP]\", \"[UNK]\"])\n",
    "    tokenizer.train_from_iterator([sentence], trainer)\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    encoding = tokenizer.encode(sentence)\n",
    "    print(\"Unigram Tokenization:\", encoding.tokens)\n",
    "    return encoding.tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Byte-Pair Encoding (BPE) comes last!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte-Pair Encoding (BPE) Tokenizer\n",
    "def bpe_tokenizer():\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    \n",
    "    # Train tokenizer on the sentence\n",
    "    trainer = trainers.BpeTrainer(vocab_size=100, special_tokens=[\"[CLS]\", \"[SEP]\", \"[UNK]\"])\n",
    "    tokenizer.train_from_iterator([sentence], trainer)\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    encoding = tokenizer.encode(sentence)\n",
    "    print(\"BPE Tokenization:\", encoding.tokens)\n",
    "    return encoding.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets run the code and put their output togather to have a comparison!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tokenizers and display results\n",
    "wordpiece_tokens = wordpiece_tokenizer()\n",
    "unigram_tokens = unigram_tokenizer()\n",
    "bpe_tokens = bpe_tokenizer()\n",
    "\n",
    "# Display the results in a table\n",
    "import pandas as pd\n",
    "\n",
    "words = sentence.split(\" \")\n",
    "df = pd.DataFrame({\n",
    "    \"Word\": words,\n",
    "    \"WordPiece\": wordpiece_tokens,\n",
    "    \"Unigram\": unigram_tokens,\n",
    "    \"BPE\": bpe_tokens\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers, Check! üéâ\n",
    "Great job finishing this chapter on Tokenizers! Now, let‚Äôs recap some key takeaways:\n",
    "\n",
    "\n",
    "After this detailed exploration into tokenization concepts, you should now:\n",
    "\n",
    "- üéØ **Understand the role of tokenization** in transforming raw text into model-ready inputs for NLP tasks.\n",
    "- üéØ **Know the key differences between common tokenization techniques**, including word-level, character-level, subword-based approaches, and their use cases.\n",
    "- üéØ **Master Byte Pair Encoding (BPE)**, including how it merges frequent subword pairs to create a more efficient vocabulary for language models.\n",
    "- üéØ **Grasp WordPiece tokenization**, which is used in models like BERT, and how it builds subwords by balancing frequency and likelihood to handle unknown words.\n",
    "- üéØ **Explore Unigram tokenization**, starting with a large vocabulary and pruning unlikely subwords to maximize the model's ability to handle diverse text inputs.\n",
    "- üéØ **Use tokenizers in practice**, understanding how to apply the tokenizer within various NLP pipelines for models like GPT, BERT, and custom-built models.\n",
    "\n",
    "You‚Äôre now fully equipped to tackle tokenization challenges and seamlessly integrate tokenization into real-world NLP applications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview\n",
    "By now, you should have a solid grasp of the fundamental concepts of tokenization‚Äîhow to split text into tokens, convert those tokens into IDs, and decode IDs back into readable text. But tokenization is only one piece of the puzzle. In the next lesson, we‚Äôll dive deeper into the essential skills required for data preprocessing and preparation during the pre-training phase of large language models (LLMs). We'll explore advanced techniques such as low-quality data filtering, redundancy removal, and privacy protection. These preprocessing steps are vital to ensure that the training data is not only clean and relevant but also ethically sound. Mastering these techniques will set the stage for building better-performing and more reliable models, and understanding how tokenization interacts with these tasks will round out your toolkit for preparing data at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "- **Exercise 1:** Compare the performance of BPE, WordPiece, and Unigram tokenization methods on a small NLP task (e.g., sentiment analysis or text classification). Analyze how each method impacts model accuracy and efficiency.\n",
    "- **Exercise 2:** Implement a custom tokenizer using one of the methods discussed in this course. Apply it to a dataset of your choice and evaluate its effectiveness in handling OOV words.\n",
    "- **Exercise 3:** Reflect on how the choice of tokenizer might impact model performance when dealing with multilingual text. Write a brief essay discussing the challenges and potential solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference and Citation\n",
    "- [Tokenizers](https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt.)\n",
    "- [BPE](https://huggingface.co/learn/nlp-course/en/chapter6/5?fw=pt)\n",
    "- [WordPiece](https://huggingface.co/learn/nlp-course/en/chapter6/6?fw=pt)\n",
    "- [Unigram](https://huggingface.co/learn/nlp-course/en/chapter6/7?fw=pt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
